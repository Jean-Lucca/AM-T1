# -*- coding: utf-8 -*-
"""AM - T1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r4rXe5jbZnh8bpfe14nlER5k3FYhkQWL
"""

# ============================================
# Dependencias
# ============================================
!pip install shap lime seaborn matplotlib scikit-learn pandas ucimlrepo --quiet

import os
import urllib.request
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score
)

import shap
from lime.lime_tabular import LimeTabularExplainer

# ============================================
# Dataset escolhido
# ============================================
local_csv = "bank-additional-full.csv"

df = pd.read_csv(local_csv, sep=";")
df["y_bin"] = df["y"].map({"no": 0, "yes": 1})
print("Shape:", df.shape)
print(df["y"].value_counts())

# ============================================
# Pré-processamento
# ============================================
cat_cols = df.select_dtypes(include=["object"]).columns.tolist()
cat_cols.remove("y")
num_cols = df.select_dtypes(include=["int64","float64"]).columns.tolist()
if "y_bin" in num_cols: num_cols.remove("y_bin")
# Para variáveis numéricas: substitui valores faltantes pela mediana + padroniza.
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])
# Para variáveis categóricas: substitui missing + aplica one-hot encoding.
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="constant", fill_value="missing")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])
# Combina transformações em um único objeto para aplicar a cada tipo de coluna.
preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, num_cols),
        ("cat", categorical_transformer, cat_cols)
    ]
)
# Separa features (X) e alvo (y).
# Divide em treino e teste (80/20), mantendo proporção da classe.
X = df.drop(columns=["y","y_bin"])
y = df["y_bin"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ============================================
# Modelos e GridSearch
# ============================================
# Validação cruzada estratificada (5 folds).
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
# Cria pipelines para cada modelo, aplicando o pré-processamento antes do classificador.
pipe_knn = Pipeline([("pre", preprocessor), ("clf", KNeighborsClassifier())])
pipe_nb  = Pipeline([("pre", preprocessor), ("clf", GaussianNB())])
pipe_dt  = Pipeline([("pre", preprocessor), ("clf", DecisionTreeClassifier(random_state=42))])
# Define grades de hiperparâmetros para buscar com GridSearch.
param_knn = {"clf__n_neighbors": [3,5,7], "clf__weights": ["uniform","distance"]}
param_nb  = {"clf__var_smoothing": [1e-9, 1e-8, 1e-7]}
param_dt  = {"clf__max_depth": [5,10,20,None], "clf__criterion": ["gini","entropy"]}
# Configura GridSearchCV para os três modelos (métrica = F1).
grid_knn = GridSearchCV(pipe_knn, param_knn, cv=cv, scoring="f1", n_jobs=-1)
grid_nb  = GridSearchCV(pipe_nb, param_nb, cv=cv, scoring="f1", n_jobs=-1)
grid_dt  = GridSearchCV(pipe_dt, param_dt, cv=cv, scoring="f1", n_jobs=-1)
# Treina os modelos e busca melhores hiperparâmetros.
print("Treinando KNN..."); grid_knn.fit(X_train, y_train)
print("Treinando NB...");  grid_nb.fit(X_train, y_train)
print("Treinando DT...");  grid_dt.fit(X_train, y_train)

# ============================================
# Avaliação
# ============================================
# Guarda os melhores modelos encontrados.
models = {
    "KNN": grid_knn.best_estimator_,
    "GaussianNB": grid_nb.best_estimator_,
    "DecisionTree": grid_dt.best_estimator_
}
# Para cada modelo:
# Faz previsões (y_pred);
# Calcula probabilidades (se possível);
# Mede métricas (accuracy, precision, recall, f1, ROC AUC);
# Mostra relatório detalhado.
results = {}
for name, model in models.items():
    y_pred = model.predict(X_test)
    try:
        y_proba = model.predict_proba(X_test)[:,1]
    except:
        y_proba = None
    results[name] = {
        "accuracy": accuracy_score(y_test, y_pred),
        "precision": precision_score(y_test, y_pred),
        "recall": recall_score(y_test, y_pred),
        "f1": f1_score(y_test, y_pred),
        "roc_auc": roc_auc_score(y_test, y_proba) if y_proba is not None else np.nan
    }
    print(f"\n{name}:\n", classification_report(y_test, y_pred))
# Cria DataFrame comparativo com métricas.
results_df = pd.DataFrame(results).T
print("\nResumo métricas:\n", results_df)

# ============================================
# Interpretabilidade - Decision Tree
# ============================================
# Ajusta pré-processador para recuperar nomes das features após one-hot encoding.
preprocessor.fit(X_train)
ohe = preprocessor.named_transformers_["cat"].named_steps["onehot"]
feature_names = num_cols + list(ohe.get_feature_names_out(cat_cols))
# Pega a árvore de decisão treinada.
best_dt = grid_dt.best_estimator_.named_steps["clf"]
# Extrai importâncias das features e mostra as 15 principais.
importances = best_dt.feature_importances_
imp_df = pd.DataFrame({"feature": feature_names, "importance": importances})
imp_df = imp_df.sort_values("importance", ascending=False).head(15)
# Plota gráfico de barras com importâncias.
plt.figure(figsize=(8,6))
sns.barplot(data=imp_df, x="importance", y="feature")
plt.title("Top 15 Importâncias (Decision Tree)")
plt.show()

# ============================================
# Interpretabilidade - KNN com LIME
# ============================================
# Transforma dados (numéricos + one-hot).
X_train_pre = preprocessor.transform(X_train)
X_test_pre  = preprocessor.transform(X_test)
# Cria explicador LIME para classificação tabular.
explainer = LimeTabularExplainer(
    training_data=np.array(X_train_pre),
    feature_names=feature_names,
    class_names=["no","yes"],
    mode="classification"
)
# Escolhe uma instância aleatória do conjunto de teste.
idx = np.random.randint(0, X_test.shape[0])
# Define função auxiliar para fornecer probabilidades do KNN já treinado.
# Define a custom prediction function that uses the trained classifier on preprocessed data
def predict_proba_knn_classifier(data_preprocessed):
    # Access the trained classifier directly from the pipeline
    return grid_knn.best_estimator_.named_steps['clf'].predict_proba(data_preprocessed)

# Explica a instância selecionada e mostra a contribuição das features.
exp = explainer.explain_instance(
    data_row=X_test_pre[idx],
    predict_fn=predict_proba_knn_classifier # Use the custom prediction function
)
print("\nLIME explicação KNN:\n", exp.as_list())
exp.show_in_notebook(show_table=True)

# ============================================
# Interpretabilidade - SHAP para Decision Tree
# ============================================
# Inicializa SHAP;
# Cria explicador otimizado para árvore de decisão;
# Calcula valores SHAP no conjunto de teste;
# Mostra gráfico de resumo com as features mais influentes.
shap.initjs()
explainer_shap = shap.TreeExplainer(best_dt)
shap_values = explainer_shap.shap_values(X_test_pre)

shap.summary_plot(shap_values, features=X_test_pre, feature_names=feature_names)